from pyspark import SparkConf, SparkContext 
from pyspark.sql import SparkSession
from pyspark.sql import SQLContext 
from pyspark.sql.functions import *
from pyspark.sql.types import *


hadoop fs -rm -r /stream/

hadoop fs -mkdir /stream/

hadoop fs -put cust_1.txt  /stream/

hadoop fs -put cust_2.txt  /stream/

hadoop fs -put cust_3.txt  /stream/


spark = SparkSession \
    .builder \
    .appName("user-logs-analysis-streaming") \
    .getOrCreate()

schema1 = StructType([ \
        StructField("custid", IntegerType(), True), \
        StructField("custfname", StringType(), True), \
        StructField("custlname", StringType(), True), \
        StructField("custage", StringType(), True), \
        StructField("custprofession", StringType(), True), \
        ])

df = spark \
      .readStream \
      .format("csv") \
      .option("header", "true") \
      .option("inferSchema", True) \
      .option("maxFilesPerTrigger", 1) \
      .schema(schema1) \
      .load("/stream/")

df.printSchema()

df.show(10,False)

case1:

df.writeStream.format("console").outputMode("append").start().awaitTermination()

case 2:

df2=df.groupBy("custprofession").agg(avg("custage").alias("avg_age"), count("custid").alias("customer_count"))

df2.writeStream.format("console").outputMode("complete").start().awaitTermination()
